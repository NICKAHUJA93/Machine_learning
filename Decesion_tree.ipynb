{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris data sets are {'data': array([[5.1, 3.5, 1.4, 0.2],\n",
      "       [4.9, 3. , 1.4, 0.2],\n",
      "       [4.7, 3.2, 1.3, 0.2],\n",
      "       [4.6, 3.1, 1.5, 0.2],\n",
      "       [5. , 3.6, 1.4, 0.2],\n",
      "       [5.4, 3.9, 1.7, 0.4],\n",
      "       [4.6, 3.4, 1.4, 0.3],\n",
      "       [5. , 3.4, 1.5, 0.2],\n",
      "       [4.4, 2.9, 1.4, 0.2],\n",
      "       [4.9, 3.1, 1.5, 0.1],\n",
      "       [5.4, 3.7, 1.5, 0.2],\n",
      "       [4.8, 3.4, 1.6, 0.2],\n",
      "       [4.8, 3. , 1.4, 0.1],\n",
      "       [4.3, 3. , 1.1, 0.1],\n",
      "       [5.8, 4. , 1.2, 0.2],\n",
      "       [5.7, 4.4, 1.5, 0.4],\n",
      "       [5.4, 3.9, 1.3, 0.4],\n",
      "       [5.1, 3.5, 1.4, 0.3],\n",
      "       [5.7, 3.8, 1.7, 0.3],\n",
      "       [5.1, 3.8, 1.5, 0.3],\n",
      "       [5.4, 3.4, 1.7, 0.2],\n",
      "       [5.1, 3.7, 1.5, 0.4],\n",
      "       [4.6, 3.6, 1. , 0.2],\n",
      "       [5.1, 3.3, 1.7, 0.5],\n",
      "       [4.8, 3.4, 1.9, 0.2],\n",
      "       [5. , 3. , 1.6, 0.2],\n",
      "       [5. , 3.4, 1.6, 0.4],\n",
      "       [5.2, 3.5, 1.5, 0.2],\n",
      "       [5.2, 3.4, 1.4, 0.2],\n",
      "       [4.7, 3.2, 1.6, 0.2],\n",
      "       [4.8, 3.1, 1.6, 0.2],\n",
      "       [5.4, 3.4, 1.5, 0.4],\n",
      "       [5.2, 4.1, 1.5, 0.1],\n",
      "       [5.5, 4.2, 1.4, 0.2],\n",
      "       [4.9, 3.1, 1.5, 0.2],\n",
      "       [5. , 3.2, 1.2, 0.2],\n",
      "       [5.5, 3.5, 1.3, 0.2],\n",
      "       [4.9, 3.6, 1.4, 0.1],\n",
      "       [4.4, 3. , 1.3, 0.2],\n",
      "       [5.1, 3.4, 1.5, 0.2],\n",
      "       [5. , 3.5, 1.3, 0.3],\n",
      "       [4.5, 2.3, 1.3, 0.3],\n",
      "       [4.4, 3.2, 1.3, 0.2],\n",
      "       [5. , 3.5, 1.6, 0.6],\n",
      "       [5.1, 3.8, 1.9, 0.4],\n",
      "       [4.8, 3. , 1.4, 0.3],\n",
      "       [5.1, 3.8, 1.6, 0.2],\n",
      "       [4.6, 3.2, 1.4, 0.2],\n",
      "       [5.3, 3.7, 1.5, 0.2],\n",
      "       [5. , 3.3, 1.4, 0.2],\n",
      "       [7. , 3.2, 4.7, 1.4],\n",
      "       [6.4, 3.2, 4.5, 1.5],\n",
      "       [6.9, 3.1, 4.9, 1.5],\n",
      "       [5.5, 2.3, 4. , 1.3],\n",
      "       [6.5, 2.8, 4.6, 1.5],\n",
      "       [5.7, 2.8, 4.5, 1.3],\n",
      "       [6.3, 3.3, 4.7, 1.6],\n",
      "       [4.9, 2.4, 3.3, 1. ],\n",
      "       [6.6, 2.9, 4.6, 1.3],\n",
      "       [5.2, 2.7, 3.9, 1.4],\n",
      "       [5. , 2. , 3.5, 1. ],\n",
      "       [5.9, 3. , 4.2, 1.5],\n",
      "       [6. , 2.2, 4. , 1. ],\n",
      "       [6.1, 2.9, 4.7, 1.4],\n",
      "       [5.6, 2.9, 3.6, 1.3],\n",
      "       [6.7, 3.1, 4.4, 1.4],\n",
      "       [5.6, 3. , 4.5, 1.5],\n",
      "       [5.8, 2.7, 4.1, 1. ],\n",
      "       [6.2, 2.2, 4.5, 1.5],\n",
      "       [5.6, 2.5, 3.9, 1.1],\n",
      "       [5.9, 3.2, 4.8, 1.8],\n",
      "       [6.1, 2.8, 4. , 1.3],\n",
      "       [6.3, 2.5, 4.9, 1.5],\n",
      "       [6.1, 2.8, 4.7, 1.2],\n",
      "       [6.4, 2.9, 4.3, 1.3],\n",
      "       [6.6, 3. , 4.4, 1.4],\n",
      "       [6.8, 2.8, 4.8, 1.4],\n",
      "       [6.7, 3. , 5. , 1.7],\n",
      "       [6. , 2.9, 4.5, 1.5],\n",
      "       [5.7, 2.6, 3.5, 1. ],\n",
      "       [5.5, 2.4, 3.8, 1.1],\n",
      "       [5.5, 2.4, 3.7, 1. ],\n",
      "       [5.8, 2.7, 3.9, 1.2],\n",
      "       [6. , 2.7, 5.1, 1.6],\n",
      "       [5.4, 3. , 4.5, 1.5],\n",
      "       [6. , 3.4, 4.5, 1.6],\n",
      "       [6.7, 3.1, 4.7, 1.5],\n",
      "       [6.3, 2.3, 4.4, 1.3],\n",
      "       [5.6, 3. , 4.1, 1.3],\n",
      "       [5.5, 2.5, 4. , 1.3],\n",
      "       [5.5, 2.6, 4.4, 1.2],\n",
      "       [6.1, 3. , 4.6, 1.4],\n",
      "       [5.8, 2.6, 4. , 1.2],\n",
      "       [5. , 2.3, 3.3, 1. ],\n",
      "       [5.6, 2.7, 4.2, 1.3],\n",
      "       [5.7, 3. , 4.2, 1.2],\n",
      "       [5.7, 2.9, 4.2, 1.3],\n",
      "       [6.2, 2.9, 4.3, 1.3],\n",
      "       [5.1, 2.5, 3. , 1.1],\n",
      "       [5.7, 2.8, 4.1, 1.3],\n",
      "       [6.3, 3.3, 6. , 2.5],\n",
      "       [5.8, 2.7, 5.1, 1.9],\n",
      "       [7.1, 3. , 5.9, 2.1],\n",
      "       [6.3, 2.9, 5.6, 1.8],\n",
      "       [6.5, 3. , 5.8, 2.2],\n",
      "       [7.6, 3. , 6.6, 2.1],\n",
      "       [4.9, 2.5, 4.5, 1.7],\n",
      "       [7.3, 2.9, 6.3, 1.8],\n",
      "       [6.7, 2.5, 5.8, 1.8],\n",
      "       [7.2, 3.6, 6.1, 2.5],\n",
      "       [6.5, 3.2, 5.1, 2. ],\n",
      "       [6.4, 2.7, 5.3, 1.9],\n",
      "       [6.8, 3. , 5.5, 2.1],\n",
      "       [5.7, 2.5, 5. , 2. ],\n",
      "       [5.8, 2.8, 5.1, 2.4],\n",
      "       [6.4, 3.2, 5.3, 2.3],\n",
      "       [6.5, 3. , 5.5, 1.8],\n",
      "       [7.7, 3.8, 6.7, 2.2],\n",
      "       [7.7, 2.6, 6.9, 2.3],\n",
      "       [6. , 2.2, 5. , 1.5],\n",
      "       [6.9, 3.2, 5.7, 2.3],\n",
      "       [5.6, 2.8, 4.9, 2. ],\n",
      "       [7.7, 2.8, 6.7, 2. ],\n",
      "       [6.3, 2.7, 4.9, 1.8],\n",
      "       [6.7, 3.3, 5.7, 2.1],\n",
      "       [7.2, 3.2, 6. , 1.8],\n",
      "       [6.2, 2.8, 4.8, 1.8],\n",
      "       [6.1, 3. , 4.9, 1.8],\n",
      "       [6.4, 2.8, 5.6, 2.1],\n",
      "       [7.2, 3. , 5.8, 1.6],\n",
      "       [7.4, 2.8, 6.1, 1.9],\n",
      "       [7.9, 3.8, 6.4, 2. ],\n",
      "       [6.4, 2.8, 5.6, 2.2],\n",
      "       [6.3, 2.8, 5.1, 1.5],\n",
      "       [6.1, 2.6, 5.6, 1.4],\n",
      "       [7.7, 3. , 6.1, 2.3],\n",
      "       [6.3, 3.4, 5.6, 2.4],\n",
      "       [6.4, 3.1, 5.5, 1.8],\n",
      "       [6. , 3. , 4.8, 1.8],\n",
      "       [6.9, 3.1, 5.4, 2.1],\n",
      "       [6.7, 3.1, 5.6, 2.4],\n",
      "       [6.9, 3.1, 5.1, 2.3],\n",
      "       [5.8, 2.7, 5.1, 1.9],\n",
      "       [6.8, 3.2, 5.9, 2.3],\n",
      "       [6.7, 3.3, 5.7, 2.5],\n",
      "       [6.7, 3. , 5.2, 2.3],\n",
      "       [6.3, 2.5, 5. , 1.9],\n",
      "       [6.5, 3. , 5.2, 2. ],\n",
      "       [6.2, 3.4, 5.4, 2.3],\n",
      "       [5.9, 3. , 5.1, 1.8]]), 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]), 'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='<U10'), 'DESCR': '.. _iris_dataset:\\n\\nIris plants dataset\\n--------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n                \\n    :Summary Statistics:\\n\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\nfrom Fisher\\'s paper. Note that it\\'s the same as in R, but not as in the UCI\\nMachine Learning Repository, which has two wrong data points.\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\n.. topic:: References\\n\\n   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n     Mathematical Statistics\" (John Wiley, NY, 1950).\\n   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n     Structure and Classification Rule for Recognition in Partially Exposed\\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n     on Information Theory, May 1972, 431-433.\\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n     conceptual clustering system finds 3 classes in the data.\\n   - Many, many more ...', 'feature_names': ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)'], 'filename': 'C:\\\\Users\\\\Dell\\\\Anaconda4\\\\lib\\\\site-packages\\\\sklearn\\\\datasets\\\\data\\\\iris.csv'}\n",
      "data is split with x cordinate\n",
      " [[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.9 3.6 1.4 0.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.7 3.  5.  1.7]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]]\n",
      "data is split with y cordinate\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "Pricted value comes out to be [0 0 2 0 0 1 0 2 2 0 0 0 0 0 1 1 0 1 2 1 2 1 2 1 1 0 0 2 0 2 2 0 1 2 1 0 2\n",
      " 1]\n",
      "Accuracy of the model is 94.73684210526315\n",
      "Confusion matrix is \n",
      " [[16  0  0]\n",
      " [ 0 10  1]\n",
      " [ 0  1 10]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1, 'accuracy 0.9473684210526315 ')"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as nm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics \n",
    "data = load_iris()\n",
    "print(\"Iris data sets are\",data)\n",
    "x_data = data.data\n",
    "y_data = data.target\n",
    "print(\"data is split with x cordinate\\n\",x_data)\n",
    "print(\"data is split with y cordinate\\n\",y_data)\n",
    "x_data[0:4]\n",
    "y_data[0:4]\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, random_state = 2, test_size = 0.25)\n",
    "clf = DecisionTreeClassifier(criterion = 'entropy')\n",
    "clf.fit(x_train,y_train)\n",
    "y_pred=clf.predict(x_test)\n",
    "print(\"Pricted value comes out to be\",y_pred)\n",
    "accurcy= clf.score(x_test,y_test)\n",
    "print(\"Accuracy of the model is\",accurcy*100)\n",
    "confusion_matrix =metrics.confusion_matrix(y_test,y_pred)\n",
    "print(\"Confusion matrix is \\n\",confusion_matrix)\n",
    "#DS = DecisionTreeClassifier()\n",
    "plt.figure(figsize=(9,9))\n",
    "sns.heatmap(confusion_matrix,fmt=\".3f\",annot=True,linewidths=.5,square=True)\n",
    "plt.ylabel(\"actual data\")\n",
    "plt.xlabel(\"predicted data\")\n",
    "#Putting a title as score\n",
    "all_title=\"accuracy {0} \".format(accurcy)\n",
    "#changing a title size to 15\n",
    "plt.title(all_title,size=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wine dataset is \n",
      " {'data': array([[1.423e+01, 1.710e+00, 2.430e+00, ..., 1.040e+00, 3.920e+00,\n",
      "        1.065e+03],\n",
      "       [1.320e+01, 1.780e+00, 2.140e+00, ..., 1.050e+00, 3.400e+00,\n",
      "        1.050e+03],\n",
      "       [1.316e+01, 2.360e+00, 2.670e+00, ..., 1.030e+00, 3.170e+00,\n",
      "        1.185e+03],\n",
      "       ...,\n",
      "       [1.327e+01, 4.280e+00, 2.260e+00, ..., 5.900e-01, 1.560e+00,\n",
      "        8.350e+02],\n",
      "       [1.317e+01, 2.590e+00, 2.370e+00, ..., 6.000e-01, 1.620e+00,\n",
      "        8.400e+02],\n",
      "       [1.413e+01, 4.100e+00, 2.740e+00, ..., 6.100e-01, 1.600e+00,\n",
      "        5.600e+02]]), 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2]), 'target_names': array(['class_0', 'class_1', 'class_2'], dtype='<U7'), 'DESCR': '.. _wine_dataset:\\n\\nWine recognition dataset\\n------------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 178 (50 in each of three classes)\\n    :Number of Attributes: 13 numeric, predictive attributes and the class\\n    :Attribute Information:\\n \\t\\t- Alcohol\\n \\t\\t- Malic acid\\n \\t\\t- Ash\\n\\t\\t- Alcalinity of ash  \\n \\t\\t- Magnesium\\n\\t\\t- Total phenols\\n \\t\\t- Flavanoids\\n \\t\\t- Nonflavanoid phenols\\n \\t\\t- Proanthocyanins\\n\\t\\t- Color intensity\\n \\t\\t- Hue\\n \\t\\t- OD280/OD315 of diluted wines\\n \\t\\t- Proline\\n\\n    - class:\\n            - class_0\\n            - class_1\\n            - class_2\\n\\t\\t\\n    :Summary Statistics:\\n    \\n    ============================= ==== ===== ======= =====\\n                                   Min   Max   Mean     SD\\n    ============================= ==== ===== ======= =====\\n    Alcohol:                      11.0  14.8    13.0   0.8\\n    Malic Acid:                   0.74  5.80    2.34  1.12\\n    Ash:                          1.36  3.23    2.36  0.27\\n    Alcalinity of Ash:            10.6  30.0    19.5   3.3\\n    Magnesium:                    70.0 162.0    99.7  14.3\\n    Total Phenols:                0.98  3.88    2.29  0.63\\n    Flavanoids:                   0.34  5.08    2.03  1.00\\n    Nonflavanoid Phenols:         0.13  0.66    0.36  0.12\\n    Proanthocyanins:              0.41  3.58    1.59  0.57\\n    Colour Intensity:              1.3  13.0     5.1   2.3\\n    Hue:                          0.48  1.71    0.96  0.23\\n    OD280/OD315 of diluted wines: 1.27  4.00    2.61  0.71\\n    Proline:                       278  1680     746   315\\n    ============================= ==== ===== ======= =====\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: class_0 (59), class_1 (71), class_2 (48)\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThis is a copy of UCI ML Wine recognition datasets.\\nhttps://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\\n\\nThe data is the results of a chemical analysis of wines grown in the same\\nregion in Italy by three different cultivators. There are thirteen different\\nmeasurements taken for different constituents found in the three types of\\nwine.\\n\\nOriginal Owners: \\n\\nForina, M. et al, PARVUS - \\nAn Extendible Package for Data Exploration, Classification and Correlation. \\nInstitute of Pharmaceutical and Food Analysis and Technologies,\\nVia Brigata Salerno, 16147 Genoa, Italy.\\n\\nCitation:\\n\\nLichman, M. (2013). UCI Machine Learning Repository\\n[https://archive.ics.uci.edu/ml]. Irvine, CA: University of California,\\nSchool of Information and Computer Science. \\n\\n.. topic:: References\\n\\n  (1) S. Aeberhard, D. Coomans and O. de Vel, \\n  Comparison of Classifiers in High Dimensional Settings, \\n  Tech. Rep. no. 92-02, (1992), Dept. of Computer Science and Dept. of  \\n  Mathematics and Statistics, James Cook University of North Queensland. \\n  (Also submitted to Technometrics). \\n\\n  The data was used with many others for comparing various \\n  classifiers. The classes are separable, though only RDA \\n  has achieved 100% correct classification. \\n  (RDA : 100%, QDA 99.4%, LDA 98.9%, 1NN 96.1% (z-transformed data)) \\n  (All results using the leave-one-out technique) \\n\\n  (2) S. Aeberhard, D. Coomans and O. de Vel, \\n  \"THE CLASSIFICATION PERFORMANCE OF RDA\" \\n  Tech. Rep. no. 92-01, (1992), Dept. of Computer Science and Dept. of \\n  Mathematics and Statistics, James Cook University of North Queensland. \\n  (Also submitted to Journal of Chemometrics).\\n', 'feature_names': ['alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium', 'total_phenols', 'flavanoids', 'nonflavanoid_phenols', 'proanthocyanins', 'color_intensity', 'hue', 'od280/od315_of_diluted_wines', 'proline']}\n",
      "X data is \n",
      " [[1.423e+01 1.710e+00 2.430e+00 ... 1.040e+00 3.920e+00 1.065e+03]\n",
      " [1.320e+01 1.780e+00 2.140e+00 ... 1.050e+00 3.400e+00 1.050e+03]\n",
      " [1.316e+01 2.360e+00 2.670e+00 ... 1.030e+00 3.170e+00 1.185e+03]\n",
      " ...\n",
      " [1.327e+01 4.280e+00 2.260e+00 ... 5.900e-01 1.560e+00 8.350e+02]\n",
      " [1.317e+01 2.590e+00 2.370e+00 ... 6.000e-01 1.620e+00 8.400e+02]\n",
      " [1.413e+01 4.100e+00 2.740e+00 ... 6.100e-01 1.600e+00 5.600e+02]]\n",
      "Y data is \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "X train data [[1.384e+01 4.120e+00 2.380e+00 ... 5.700e-01 1.640e+00 4.800e+02]\n",
      " [1.225e+01 3.880e+00 2.200e+00 ... 6.500e-01 2.000e+00 8.550e+02]\n",
      " [1.270e+01 3.870e+00 2.400e+00 ... 1.190e+00 3.130e+00 4.630e+02]\n",
      " ...\n",
      " [1.349e+01 1.660e+00 2.240e+00 ... 9.800e-01 2.780e+00 4.720e+02]\n",
      " [1.363e+01 1.810e+00 2.700e+00 ... 1.280e+00 2.880e+00 1.310e+03]\n",
      " [1.358e+01 2.580e+00 2.690e+00 ... 7.400e-01 1.800e+00 7.500e+02]]\n",
      "accuracy of the model is \n",
      " 92.5\n",
      "Confusion matrix is \n",
      " [[17  1  0]\n",
      " [ 0 12  0]\n",
      " [ 0  2  8]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Must pass 2-d input",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-82-751bd40dffd4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Confusion matrix is \\n\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfmt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\".3f\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mannot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlinewidths\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m.5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msquare\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"actual data\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"predicted data\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda4\\lib\\site-packages\\seaborn\\matrix.py\u001b[0m in \u001b[0;36mheatmap\u001b[1;34m(data, vmin, vmax, cmap, center, robust, annot, fmt, annot_kws, linewidths, linecolor, cbar, cbar_kws, cbar_ax, square, xticklabels, yticklabels, mask, ax, **kwargs)\u001b[0m\n\u001b[0;32m    515\u001b[0m     plotter = _HeatMapper(data, vmin, vmax, cmap, center, robust, annot, fmt,\n\u001b[0;32m    516\u001b[0m                           \u001b[0mannot_kws\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcbar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcbar_kws\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxticklabels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 517\u001b[1;33m                           yticklabels, mask)\n\u001b[0m\u001b[0;32m    518\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m     \u001b[1;31m# Add the pcolormesh kwargs here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda4\\lib\\site-packages\\seaborn\\matrix.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, vmin, vmax, cmap, center, robust, annot, fmt, annot_kws, cbar, cbar_kws, xticklabels, yticklabels, mask)\u001b[0m\n\u001b[0;32m    108\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m             \u001b[0mplot_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mplot_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m         \u001b[1;31m# Validate the mask and convet to DataFrame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda4\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    422\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m                 mgr = init_ndarray(data, index, columns, dtype=dtype,\n\u001b[1;32m--> 424\u001b[1;33m                                    copy=copy)\n\u001b[0m\u001b[0;32m    425\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m         \u001b[1;31m# For data is list-like, or Iterable (will consume into list)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda4\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36minit_ndarray\u001b[1;34m(values, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[1;31m# by definition an array here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[1;31m# the dtypes will be coerced to a single dtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m     \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprep_ndarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda4\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36mprep_ndarray\u001b[1;34m(values, copy)\u001b[0m\n\u001b[0;32m    247\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 249\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Must pass 2-d input'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Must pass 2-d input"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 648x648 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Implement decesion tree on wine data set and check the accuray with 60:40 test and train data size\n",
    "import numpy as nm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as mt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn import metrics\n",
    "data = load_wine()\n",
    "print(\"Wine dataset is \\n\",data)\n",
    "data_x = data.data\n",
    "print(\"X data is \\n\",data_x)\n",
    "data_y = data.target\n",
    "print(\"Y data is \\n\",data_y)\n",
    "x_train,x_test,y_train,y_test=train_test_split(data_x,data_y,test_size = 40 ,random_state =2)\n",
    "print(\"X train data\",x_train)\n",
    "decesion_tree = DecisionTreeClassifier(criterion='entropy')\n",
    "decesion_tree.fit(x_train,y_train)\n",
    "y_pred=decesion_tree.predict(x_test)\n",
    "accuracy=decesion_tree.score(x_test,y_test)\n",
    "print(\"accuracy of the model is \\n\",100*accuracy)\n",
    "cm = metrics.confusion_matrix(y_test,y_pred)\n",
    "print(\"Confusion matrix is \\n\",cm)\n",
    "plt.figure(figsize=(9,9))\n",
    "sns.heatmap(confusion_matrix,fmt=\".3f\",annot=True,linewidths=.5,square=True)\n",
    "plt.ylabel(\"actual data\")\n",
    "plt.xlabel(\"predicted data\")\n",
    "#Putting a title as score\n",
    "all_title=\"accuracy {0} \".format(accurcy)\n",
    "#changing a title size to 15\n",
    "plt.title(all_title,size=15)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diabetes data set is \n",
      " {'data': array([[ 0.03807591,  0.05068012,  0.06169621, ..., -0.00259226,\n",
      "         0.01990842, -0.01764613],\n",
      "       [-0.00188202, -0.04464164, -0.05147406, ..., -0.03949338,\n",
      "        -0.06832974, -0.09220405],\n",
      "       [ 0.08529891,  0.05068012,  0.04445121, ..., -0.00259226,\n",
      "         0.00286377, -0.02593034],\n",
      "       ...,\n",
      "       [ 0.04170844,  0.05068012, -0.01590626, ..., -0.01107952,\n",
      "        -0.04687948,  0.01549073],\n",
      "       [-0.04547248, -0.04464164,  0.03906215, ...,  0.02655962,\n",
      "         0.04452837, -0.02593034],\n",
      "       [-0.04547248, -0.04464164, -0.0730303 , ..., -0.03949338,\n",
      "        -0.00421986,  0.00306441]]), 'target': array([151.,  75., 141., 206., 135.,  97., 138.,  63., 110., 310., 101.,\n",
      "        69., 179., 185., 118., 171., 166., 144.,  97., 168.,  68.,  49.,\n",
      "        68., 245., 184., 202., 137.,  85., 131., 283., 129.,  59., 341.,\n",
      "        87.,  65., 102., 265., 276., 252.,  90., 100.,  55.,  61.,  92.,\n",
      "       259.,  53., 190., 142.,  75., 142., 155., 225.,  59., 104., 182.,\n",
      "       128.,  52.,  37., 170., 170.,  61., 144.,  52., 128.,  71., 163.,\n",
      "       150.,  97., 160., 178.,  48., 270., 202., 111.,  85.,  42., 170.,\n",
      "       200., 252., 113., 143.,  51.,  52., 210.,  65., 141.,  55., 134.,\n",
      "        42., 111.,  98., 164.,  48.,  96.,  90., 162., 150., 279.,  92.,\n",
      "        83., 128., 102., 302., 198.,  95.,  53., 134., 144., 232.,  81.,\n",
      "       104.,  59., 246., 297., 258., 229., 275., 281., 179., 200., 200.,\n",
      "       173., 180.,  84., 121., 161.,  99., 109., 115., 268., 274., 158.,\n",
      "       107.,  83., 103., 272.,  85., 280., 336., 281., 118., 317., 235.,\n",
      "        60., 174., 259., 178., 128.,  96., 126., 288.,  88., 292.,  71.,\n",
      "       197., 186.,  25.,  84.,  96., 195.,  53., 217., 172., 131., 214.,\n",
      "        59.,  70., 220., 268., 152.,  47.,  74., 295., 101., 151., 127.,\n",
      "       237., 225.,  81., 151., 107.,  64., 138., 185., 265., 101., 137.,\n",
      "       143., 141.,  79., 292., 178.,  91., 116.,  86., 122.,  72., 129.,\n",
      "       142.,  90., 158.,  39., 196., 222., 277.,  99., 196., 202., 155.,\n",
      "        77., 191.,  70.,  73.,  49.,  65., 263., 248., 296., 214., 185.,\n",
      "        78.,  93., 252., 150.,  77., 208.,  77., 108., 160.,  53., 220.,\n",
      "       154., 259.,  90., 246., 124.,  67.,  72., 257., 262., 275., 177.,\n",
      "        71.,  47., 187., 125.,  78.,  51., 258., 215., 303., 243.,  91.,\n",
      "       150., 310., 153., 346.,  63.,  89.,  50.,  39., 103., 308., 116.,\n",
      "       145.,  74.,  45., 115., 264.,  87., 202., 127., 182., 241.,  66.,\n",
      "        94., 283.,  64., 102., 200., 265.,  94., 230., 181., 156., 233.,\n",
      "        60., 219.,  80.,  68., 332., 248.,  84., 200.,  55.,  85.,  89.,\n",
      "        31., 129.,  83., 275.,  65., 198., 236., 253., 124.,  44., 172.,\n",
      "       114., 142., 109., 180., 144., 163., 147.,  97., 220., 190., 109.,\n",
      "       191., 122., 230., 242., 248., 249., 192., 131., 237.,  78., 135.,\n",
      "       244., 199., 270., 164.,  72.,  96., 306.,  91., 214.,  95., 216.,\n",
      "       263., 178., 113., 200., 139., 139.,  88., 148.,  88., 243.,  71.,\n",
      "        77., 109., 272.,  60.,  54., 221.,  90., 311., 281., 182., 321.,\n",
      "        58., 262., 206., 233., 242., 123., 167.,  63., 197.,  71., 168.,\n",
      "       140., 217., 121., 235., 245.,  40.,  52., 104., 132.,  88.,  69.,\n",
      "       219.,  72., 201., 110.,  51., 277.,  63., 118.,  69., 273., 258.,\n",
      "        43., 198., 242., 232., 175.,  93., 168., 275., 293., 281.,  72.,\n",
      "       140., 189., 181., 209., 136., 261., 113., 131., 174., 257.,  55.,\n",
      "        84.,  42., 146., 212., 233.,  91., 111., 152., 120.,  67., 310.,\n",
      "        94., 183.,  66., 173.,  72.,  49.,  64.,  48., 178., 104., 132.,\n",
      "       220.,  57.]), 'DESCR': '.. _diabetes_dataset:\\n\\nDiabetes dataset\\n----------------\\n\\nTen baseline variables, age, sex, body mass index, average blood\\npressure, and six blood serum measurements were obtained for each of n =\\n442 diabetes patients, as well as the response of interest, a\\nquantitative measure of disease progression one year after baseline.\\n\\n**Data Set Characteristics:**\\n\\n  :Number of Instances: 442\\n\\n  :Number of Attributes: First 10 columns are numeric predictive values\\n\\n  :Target: Column 11 is a quantitative measure of disease progression one year after baseline\\n\\n  :Attribute Information:\\n      - Age\\n      - Sex\\n      - Body mass index\\n      - Average blood pressure\\n      - S1\\n      - S2\\n      - S3\\n      - S4\\n      - S5\\n      - S6\\n\\nNote: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times `n_samples` (i.e. the sum of squares of each column totals 1).\\n\\nSource URL:\\nhttps://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\\n\\nFor more information see:\\nBradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \"Least Angle Regression,\" Annals of Statistics (with discussion), 407-499.\\n(https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)', 'feature_names': ['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6'], 'data_filename': 'C:\\\\Users\\\\Dell\\\\Anaconda4\\\\lib\\\\site-packages\\\\sklearn\\\\datasets\\\\data\\\\diabetes_data.csv.gz', 'target_filename': 'C:\\\\Users\\\\Dell\\\\Anaconda4\\\\lib\\\\site-packages\\\\sklearn\\\\datasets\\\\data\\\\diabetes_target.csv.gz'}\n",
      "x cordinate value of data is\n",
      " [[ 0.03807591  0.05068012  0.06169621 ... -0.00259226  0.01990842\n",
      "  -0.01764613]\n",
      " [-0.00188202 -0.04464164 -0.05147406 ... -0.03949338 -0.06832974\n",
      "  -0.09220405]\n",
      " [ 0.08529891  0.05068012  0.04445121 ... -0.00259226  0.00286377\n",
      "  -0.02593034]\n",
      " ...\n",
      " [ 0.04170844  0.05068012 -0.01590626 ... -0.01107952 -0.04687948\n",
      "   0.01549073]\n",
      " [-0.04547248 -0.04464164  0.03906215 ...  0.02655962  0.04452837\n",
      "  -0.02593034]\n",
      " [-0.04547248 -0.04464164 -0.0730303  ... -0.03949338 -0.00421986\n",
      "   0.00306441]]\n",
      "y cordinate value of data is\n",
      " [151.  75. 141. 206. 135.  97. 138.  63. 110. 310. 101.  69. 179. 185.\n",
      " 118. 171. 166. 144.  97. 168.  68.  49.  68. 245. 184. 202. 137.  85.\n",
      " 131. 283. 129.  59. 341.  87.  65. 102. 265. 276. 252.  90. 100.  55.\n",
      "  61.  92. 259.  53. 190. 142.  75. 142. 155. 225.  59. 104. 182. 128.\n",
      "  52.  37. 170. 170.  61. 144.  52. 128.  71. 163. 150.  97. 160. 178.\n",
      "  48. 270. 202. 111.  85.  42. 170. 200. 252. 113. 143.  51.  52. 210.\n",
      "  65. 141.  55. 134.  42. 111.  98. 164.  48.  96.  90. 162. 150. 279.\n",
      "  92.  83. 128. 102. 302. 198.  95.  53. 134. 144. 232.  81. 104.  59.\n",
      " 246. 297. 258. 229. 275. 281. 179. 200. 200. 173. 180.  84. 121. 161.\n",
      "  99. 109. 115. 268. 274. 158. 107.  83. 103. 272.  85. 280. 336. 281.\n",
      " 118. 317. 235.  60. 174. 259. 178. 128.  96. 126. 288.  88. 292.  71.\n",
      " 197. 186.  25.  84.  96. 195.  53. 217. 172. 131. 214.  59.  70. 220.\n",
      " 268. 152.  47.  74. 295. 101. 151. 127. 237. 225.  81. 151. 107.  64.\n",
      " 138. 185. 265. 101. 137. 143. 141.  79. 292. 178.  91. 116.  86. 122.\n",
      "  72. 129. 142.  90. 158.  39. 196. 222. 277.  99. 196. 202. 155.  77.\n",
      " 191.  70.  73.  49.  65. 263. 248. 296. 214. 185.  78.  93. 252. 150.\n",
      "  77. 208.  77. 108. 160.  53. 220. 154. 259.  90. 246. 124.  67.  72.\n",
      " 257. 262. 275. 177.  71.  47. 187. 125.  78.  51. 258. 215. 303. 243.\n",
      "  91. 150. 310. 153. 346.  63.  89.  50.  39. 103. 308. 116. 145.  74.\n",
      "  45. 115. 264.  87. 202. 127. 182. 241.  66.  94. 283.  64. 102. 200.\n",
      " 265.  94. 230. 181. 156. 233.  60. 219.  80.  68. 332. 248.  84. 200.\n",
      "  55.  85.  89.  31. 129.  83. 275.  65. 198. 236. 253. 124.  44. 172.\n",
      " 114. 142. 109. 180. 144. 163. 147.  97. 220. 190. 109. 191. 122. 230.\n",
      " 242. 248. 249. 192. 131. 237.  78. 135. 244. 199. 270. 164.  72.  96.\n",
      " 306.  91. 214.  95. 216. 263. 178. 113. 200. 139. 139.  88. 148.  88.\n",
      " 243.  71.  77. 109. 272.  60.  54. 221.  90. 311. 281. 182. 321.  58.\n",
      " 262. 206. 233. 242. 123. 167.  63. 197.  71. 168. 140. 217. 121. 235.\n",
      " 245.  40.  52. 104. 132.  88.  69. 219.  72. 201. 110.  51. 277.  63.\n",
      " 118.  69. 273. 258.  43. 198. 242. 232. 175.  93. 168. 275. 293. 281.\n",
      "  72. 140. 189. 181. 209. 136. 261. 113. 131. 174. 257.  55.  84.  42.\n",
      " 146. 212. 233.  91. 111. 152. 120.  67. 310.  94. 183.  66. 173.  72.\n",
      "  49.  64.  48. 178. 104. 132. 220.  57.]\n",
      "Pridicted values are\n",
      " [ 66. 164. 262. 168. 252.  71.  53. 164.  91. 147.  72. 252.  53.  90.\n",
      " 252.  99.  60.  48.  55. 262.  45.  79. 127. 235. 262.  55. 252. 147.\n",
      " 214. 252.  55. 127. 127.  79. 262. 127. 252.  60. 259.  53.  55. 202.\n",
      " 196. 259. 164. 252. 161. 147. 147. 107. 192.  71. 259. 115. 182.  52.\n",
      "  59. 262. 272. 297.  72. 219. 262. 143.  79. 252. 252. 163. 127. 178.\n",
      " 346. 293. 179.  74. 131. 143.  90. 189. 164. 232. 127.  54. 120. 262.\n",
      " 202. 225. 214.  45.  99.  55. 164.  90. 246.  71. 143.  99. 110. 296.\n",
      "  74.  53. 200.  89.  71.  79.  48. 272. 131.  55. 202. 201. 192. 219.\n",
      " 167.  72. 248. 127. 262. 161. 114. 232. 128. 102.  48.  72.  53. 161.\n",
      " 164.  72. 214. 161. 118. 127.  48. 262. 178.  53.  48. 124. 296. 127.\n",
      "  99. 168. 131.  53. 183. 179.  72. 143. 262. 346. 110. 143. 219.  71.\n",
      " 175. 232. 219.  53. 107.  72. 107.  43. 143. 346. 196. 102.  79. 246.\n",
      " 200. 280. 346. 252.  72. 346. 127.  48.  71.  72. 174. 110. 246. 168.\n",
      " 104. 295.  53.  55.  79.  47. 202.  71. 262. 235. 296. 202. 127. 295.\n",
      " 191.  48. 147. 214. 189.  48. 182. 104.  90. 232. 202.  72. 262. 141.\n",
      "  71. 262. 145. 262.  55. 259. 179.  97. 321.  65. 178.  53. 296. 147.\n",
      " 219.  71. 249. 232. 202. 219. 178. 147.  79. 143. 248. 259.  54. 129.\n",
      " 346. 265. 219. 111. 110. 198. 110.  72.  43. 178. 141. 235.  79.  96.\n",
      " 252.  53.  97.  72. 164.  53. 179. 296.  79. 196. 208.  52. 167. 168.\n",
      " 262. 189. 262. 297.  48.  72. 189.  90.  90.  59.  43.  71. 202. 214.\n",
      "  53. 262.  72.  72. 164. 147. 179. 185. 252. 202.  55. 270.  71.  59.\n",
      " 191.  52.  55. 296.  72. 160. 259. 110. 143.  55.  55. 143. 168. 293.\n",
      " 252.  96.]\n",
      "accuracy of the model is \n",
      " 0.967741935483871\n",
      "Confusion matrix is \n",
      " [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Apply Decesion tree on diabities data set with 50:50 train and test data size and figure out the accuracy \n",
    "import numpy as nm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as mat\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.datasets import load_diabetes\n",
    "data = load_diabetes()\n",
    "print(\"Diabetes data set is \\n\",data)\n",
    "data_x = data.data\n",
    "data_y = data.target\n",
    "print(\"x cordinate value of data is\\n\",data_x)\n",
    "print(\"y cordinate value of data is\\n\",data_y)\n",
    "# convert the data set into data frame\n",
    "#data_frame = pd.DataFrame(data.data,columns=data.feature_names)\n",
    "#print(\"Data frame....\\n\",data_frame.head())\n",
    "x_train,x_test,y_train,y_test = train_test_split(data_x,data_y,random_state=5,test_size=.70)\n",
    "dt=DecisionTreeClassifier(criterion='entropy')\n",
    "dt.fit(x_train,y_train)\n",
    "y_pred = dt.predict(x_test)\n",
    "print(\"Pridicted values are\\n\",y_pred)\n",
    "accuracy = dt.score(x_test,y_test)\n",
    "print(\"accuracy of the model is \\n\",accuracy*100)\n",
    "cm = metrics.confusion_matrix(y_test,y_pred)\n",
    "print(\"Confusion matrix is \\n\",cm)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
